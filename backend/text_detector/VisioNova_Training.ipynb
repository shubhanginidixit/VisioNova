{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# VisioNova AI Detector Training\n",
                "\n",
                "This notebook trains a DeBERTa-v3-base model for AI text detection.\n",
                "It saves the model directly to your Google Drive."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Setup Environment & Drive\n",
                "import os\n",
                "from google.colab import drive\n",
                "\n",
                "# Disable W&B logging to prevent blocking prompts\n",
                "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
                "\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "!pip install transformers datasets scikit-learn accelerate sentencepiece pandas"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "from datasets import load_dataset\n",
                "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
                "import torch\n",
                "\n",
                "# Settings\n",
                "MODEL_NAME = \"microsoft/deberta-v3-base\"\n",
                "DATASET_NAME = \"artem9k/ai-text-detection-pile\"\n",
                "MAX_SAMPLES = 20000\n",
                "EPOCHS = 3\n",
                "BATCH_SIZE = 8\n",
                "\n",
                "# Save directly to Drive\n",
                "OUTPUT_DIR = \"/content/drive/MyDrive/VisioNova_Model\"\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "print(f\"Model will be saved to: {OUTPUT_DIR}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Download and Balance Data\n",
                "print(\"Downloading dataset...\")\n",
                "dataset = load_dataset(DATASET_NAME, split=\"train\", streaming=True)\n",
                "\n",
                "rows = []\n",
                "counts = {0: 0, 1: 0}\n",
                "target = MAX_SAMPLES // 2\n",
                "\n",
                "print(\"Balancing...\")\n",
                "for item in dataset:\n",
                "    text = item.get('text', item.get('content', ''))\n",
                "    if not text or len(text) < 50: continue\n",
                "    \n",
                "    label = item.get('label')\n",
                "    if label is None:\n",
                "        if 'generated' in item: label = 1 if item['generated'] else 0\n",
                "        elif 'source' in item and 'ai' in str(item['source']).lower(): label = 1\n",
                "        else: continue\n",
                "    \n",
                "    label = int(label)\n",
                "    if counts[label] < target:\n",
                "        rows.append({'text': text, 'label': label})\n",
                "        counts[label] += 1\n",
                "    if counts[0] >= target and counts[1] >= target: break\n",
                "\n",
                "df = pd.DataFrame(rows)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Train\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
                "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
                "\n",
                "train_df, val_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n",
                "\n",
                "def tokenize(df):\n",
                "    texts = df['text'].tolist()\n",
                "    labels = df['label'].tolist()\n",
                "    encs = tokenizer(texts, truncation=True, padding=True, max_length=512)\n",
                "    class DS(torch.utils.data.Dataset):\n",
                "        def __init__(self, encs, labs): self.encs, self.labs = encs, labs\n",
                "        def __getitem__(self, i): \n",
                "            item = {k: torch.tensor(v[i]) for k, v in self.encs.items()}\n",
                "            item['labels'] = torch.tensor(self.labs[i])\n",
                "            return item\n",
                "        def __len__(self): return len(self.labs)\n",
                "    return DS(encs, labels)\n",
                "\n",
                "train_ds = tokenize(train_df)\n",
                "val_ds = tokenize(val_df)\n",
                "\n",
                "def metrics(pred):\n",
                "    labels = pred.label_ids\n",
                "    preds = pred.predictions.argmax(-1)\n",
                "    acc = accuracy_score(labels, preds)\n",
                "    return {'accuracy': acc}\n",
                "\n",
                "args = TrainingArguments(\n",
                "    output_dir=OUTPUT_DIR,\n",
                "    num_train_epochs=EPOCHS,\n",
                "    per_device_train_batch_size=BATCH_SIZE,\n",
                "    warmup_ratio=0.1,\n",
                "    fp16=torch.cuda.is_available(),\n",
                "    report_to=\"none\"  # Disable W&B logging\n",
                ")\n",
                "\n",
                "trainer = Trainer(model, args, train_dataset=train_ds, eval_dataset=val_ds, compute_metrics=metrics)\n",
                "trainer.train()\n",
                "trainer.save_model(OUTPUT_DIR)\n",
                "tokenizer.save_pretrained(OUTPUT_DIR)\n",
                "print(f\"DONE! Model saved to {OUTPUT_DIR}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}