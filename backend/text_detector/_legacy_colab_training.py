# -*- coding: utf-8 -*-
"""VisioNova_DeBERTa_Training.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/VisioNova_Training

# VisioNova: DeBERTa-v3 AI Text Detection Training
This notebook trains the state-of-the-art `microsoft/deberta-v3-base` model on real-world datasets.

**Hardware Requirements:**
*   GPU required (T4 is sufficient, A100 preferred)
*   High-RAM preferred for large datasets
"""

# ==========================================
# 1. SETUP & DEPENDENCIES
# ==========================================
print("Installing dependencies...")
!pip install -q transformers datasets accelerate scikit-learn sentencepiece

import os
import torch
import numpy as np
from google.colab import drive
from datasets import load_dataset, concatenate_datasets
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    DataCollatorWithPadding
)
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

# ==========================================
# 2. CONFIGURATION & SAFETY CHECKS
# ==========================================
# STRICT GPU CHECK
if not torch.cuda.is_available():
    raise RuntimeError(
        "\n\nðŸ›‘ PENDING ACTION: GPU NOT DETECTED! ðŸ›‘\n"
        "1. Go to the top menu: Runtime > Change runtime type\n"
        "2. Select 'T4 GPU' (or better) from the Hardware Accelerator dropdown\n"
        "3. Click Save\n"
        "4. Restart the session and run this cell again.\n"
    )
else:
    print(f"âœ… GPU Detected: {torch.cuda.get_device_name(0)}")

# --- DATASET SELECTION ---
# Options: 'artem9k/ai-text-detection-pile', 'verusen/raid'
DATASET_NAME = "artem9k/ai-text-detection-pile"
# NOTE: This is a stable, Parquet-based dataset comprising 1.5M+ balanced samples.

# --- MODEL IO ---
MODEL_ID = "microsoft/deberta-v3-base"
DRIVE_MOUNT_PATH = "/content/drive"
SAVE_DIR_NAME = f"{DATASET_NAME.split('/')[-1]}_DeBERTa_v3"
OUTPUT_DIR = os.path.join(DRIVE_MOUNT_PATH, "MyDrive", "VisioNova_Models", SAVE_DIR_NAME)

# --- HYPERPARAMETERS (Optimized for Free Tier T4) ---
EPOCHS = 3
BATCH_SIZE = 8   # Solid for T4. If OOM, reduce to 4.
LEARNING_RATE = 2e-5
MAX_LEN = 512

print(f"Model will be saved to: {OUTPUT_DIR}")


# ==========================================
# 3. MOUNT DRIVE
# ==========================================
if not os.path.exists(DRIVE_MOUNT_PATH):
    print("Mounting Google Drive...")
    drive.mount(DRIVE_MOUNT_PATH)
else:
    print("Drive already mounted.")

# Create directory
os.makedirs(OUTPUT_DIR, exist_ok=True)

# ==========================================
# 4. LOAD & PREPARE DATASET
# ==========================================
print(f"Loading dataset: {DATASET_NAME}...")

# Generic loading for standard Parquet datasets (Text/Label)
dataset = load_dataset(DATASET_NAME)

# Inspect hierarchy
if 'train' in dataset:
    # If dataset is huge, slice it for demonstration (optional, removing limit for real training)
    # dataset['train'] = dataset['train'].select(range(100000)) # Uncomment to limit size
    split_dataset = dataset['train'].train_test_split(test_size=0.1)
else:
    split_dataset = dataset.train_test_split(test_size=0.1)

print(f"Dataset columns: {split_dataset['train'].column_names}")

# Ensure columns are 'text' and 'label'
# artem9k dataset uses 'text' and 'label' (human=0, ai=1) or similar.
# We normalize just in case.
def normalize_dataset(example):
    return example

print(f"Training Samples: {len(split_dataset['train'])}")
print(f"Validation Samples: {len(split_dataset['test'])}")


# ==========================================
# 5. TOKENIZATION & FORMATTING
# ==========================================
tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)

def tokenize_function(examples):
    return tokenizer(
        examples["text"], 
        padding="max_length", 
        truncation=True, 
        max_length=MAX_LEN
    )

print("Tokenizing dataset...")
tokenized_datasets = split_dataset.map(tokenize_function, batched=True)

# CRITICAL FIX: Ensure label column is named 'labels' for Hugging Face Trainer
# Check existing column names
cols = tokenized_datasets["train"].column_names
print(f"Available columns: {cols}")

label_col = None
for candidate in ["label", "generated", "label_id", "target"]:
    if candidate in cols:
        label_col = candidate
        break

if label_col:
    print(f"Found label column: '{label_col}'. Renaming to 'labels'...")
    tokenized_datasets = tokenized_datasets.rename_column(label_col, "labels")
else:
    print("WARNING: Could not identify a label column! Training might fail.")
    print("Please inspect the 'Available columns' above.")

# Set format for PyTorch
tokenized_datasets.set_format(type="torch", columns=["input_ids", "token_type_ids", "attention_mask", "labels"])


# ==========================================
# 6. TRAINING SETUP
# ==========================================
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')
    acc = accuracy_score(labels, predictions)
    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }

model = AutoModelForSequenceClassification.from_pretrained(
    MODEL_ID, 
    num_labels=2,
    id2label={0: "HUMAN", 1: "AI"},
    label2id={"HUMAN": 0, "AI": 1}
)

training_args = TrainingArguments(
    output_dir="./results",
    learning_rate=LEARNING_RATE,
    per_device_train_batch_size=BATCH_SIZE,
    per_device_eval_batch_size=BATCH_SIZE,
    num_train_epochs=EPOCHS,
    weight_decay=0.01,
    eval_strategy="epoch",  # Updated from evaluation_strategy
    save_strategy="epoch",
    load_best_model_at_end=True,
    fp16=True,             # Enable mixed precision for T4 GPU
    report_to="none"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["test"],
    tokenizer=tokenizer,
    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),
    compute_metrics=compute_metrics,
)

# ==========================================
# 7. SAFETY CHECK (Optional but Recommended)
# ==========================================
print("Running safety check (10 samples, 1 step)...")
try:
    # Create mini dataset for sanity check
    mini_train = tokenized_datasets["train"].select(range(10))
    mini_test = tokenized_datasets["test"].select(range(10))
    
    safety_trainer = Trainer(
        model=model,
        args=TrainingArguments(
            output_dir="./safety_check",
            max_steps=1,
            per_device_train_batch_size=2,
            logging_steps=1,
            report_to="none"
        ),
        train_dataset=mini_train,
        eval_dataset=mini_test,
        tokenizer=tokenizer,
        data_collator=DataCollatorWithPadding(tokenizer=tokenizer)
    )
    safety_trainer.train()
    print("Safety check PASSED! Proceeding to full training...")
except Exception as e:
    print(f"Safety check FAILED: {e}")
    print("ABORTING full training to save time. Please check errors above.")
    raise e

# ==========================================
# 8. TRAIN & SAVE
# ==========================================
print("Starting training...")
trainer.train()

print(f"\nSaving model to Google Drive: {OUTPUT_DIR}")
trainer.save_model(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)

# Save metrics
metrics = trainer.evaluate()
print(f"Final Metrics: {metrics}")

with open(os.path.join(OUTPUT_DIR, "metrics.txt"), "w") as f:
    f.write(str(metrics))

print("\nDONE! You can now download the folder from Drive and place it in 'backend/text_detector/model'")
