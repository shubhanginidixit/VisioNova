{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# VisioNova: DeBERTa-v3 AI Text Detection Training\n",
                "\n",
                "This notebook trains the `microsoft/deberta-v3-base` model using the Google Colab runtime connected via VS Code.\n",
                "\n",
                "**Important:**\n",
                "- This notebook runs on the remote Colab instance.\n",
                "- Models are saved to the remote filesystem.\n",
                "- You must manually download the trained model folder from the VS Code file explorer after training."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "baa5bbd6",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. SETUP & DEPENDENCIES\n",
                "print(\"Installing dependencies...\")\n",
                "!pip install -q transformers datasets accelerate scikit-learn sentencepiece"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "228b90a9",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. IMPORTS & CONFIGURATION\n",
                "import os\n",
                "import torch\n",
                "import numpy as np\n",
                "from datasets import load_dataset\n",
                "from transformers import (\n",
                "    AutoTokenizer,\n",
                "    AutoModelForSequenceClassification,\n",
                "    TrainingArguments,\n",
                "    Trainer,\n",
                "    DataCollatorWithPadding\n",
                ")\n",
                "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
                "\n",
                "# STRICT GPU CHECK\n",
                "if not torch.cuda.is_available():\n",
                "    raise RuntimeError(\n",
                "        \"\\n\\nðŸ›‘ GPU NOT DETECTED! ðŸ›‘\\n\"\n",
                "        \"Make sure you are connected to a Colab Runtime with GPU enabled (Runtime > Change runtime type > GPU).\"\n",
                "    )\n",
                "else:\n",
                "    print(f\"âœ… GPU Detected: {torch.cuda.get_device_name(0)}\")\n",
                "\n",
                "# --- CONFIG ---\n",
                "DATASET_NAME = \"artem9k/ai-text-detection-pile\"\n",
                "MODEL_ID = \"microsoft/deberta-v3-base\"\n",
                "SAVE_DIR_NAME = f\"{DATASET_NAME.split('/')[-1]}_DeBERTa_v3\"\n",
                "OUTPUT_DIR = os.path.join(\"./VisioNova_Models\", SAVE_DIR_NAME)\n",
                "\n",
                "# --- HYPERPARAMETERS ---\n",
                "EPOCHS = 3\n",
                "BATCH_SIZE = 8\n",
                "LEARNING_RATE = 2e-5\n",
                "MAX_LEN = 512\n",
                "\n",
                "print(f\"Model will be saved to: {OUTPUT_DIR}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3378a99f",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. LOAD & PREPARE DATASET\n",
                "print(f\"Loading dataset: {DATASET_NAME}...\")\n",
                "dataset = load_dataset(DATASET_NAME)\n",
                "\n",
                "# Print dataset info for debugging\n",
                "print(f\"Dataset splits: {list(dataset.keys())}\")\n",
                "sample_split = list(dataset.keys())[0]\n",
                "print(f\"Columns in '{sample_split}': {dataset[sample_split].column_names}\")\n",
                "\n",
                "if 'train' in dataset:\n",
                "    split_dataset = dataset['train'].train_test_split(test_size=0.1, seed=42)\n",
                "else:\n",
                "    split_dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
                "\n",
                "print(f\"Training Samples: {len(split_dataset['train'])}\")\n",
                "print(f\"Validation Samples: {len(split_dataset['test'])}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "52c257d4",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. CREATE LABELS FROM 'source' COLUMN\n",
                "# The dataset has 'source' column indicating origin (e.g., 'human', 'chatgpt', 'gpt-4').\n",
                "# We derive binary labels: 0 = Human, 1 = AI-generated.\n",
                "\n",
                "# First, let's see unique sources\n",
                "unique_sources = set(split_dataset['train']['source'][:10000])  # Sample first 10k\n",
                "print(f\"Sample sources found: {unique_sources}\")\n",
                "\n",
                "# Define human sources (add more if needed based on output above)\n",
                "HUMAN_SOURCES = {'human', 'Human', 'wikipedia', 'reddit', 'news', 'books'}\n",
                "\n",
                "def add_labels(example):\n",
                "    \"\"\"Add binary label based on source column.\"\"\"\n",
                "    source = example.get('source', '').lower()\n",
                "    # If source contains any human indicator, label as 0 (human)\n",
                "    is_human = any(h.lower() in source for h in HUMAN_SOURCES)\n",
                "    example['labels'] = 0 if is_human else 1\n",
                "    return example\n",
                "\n",
                "print(\"Adding labels based on 'source' column...\")\n",
                "split_dataset = split_dataset.map(add_labels)\n",
                "\n",
                "# Verify label distribution\n",
                "train_labels = split_dataset['train']['labels']\n",
                "human_count = sum(1 for l in train_labels if l == 0)\n",
                "ai_count = sum(1 for l in train_labels if l == 1)\n",
                "print(f\"Label distribution - Human: {human_count}, AI: {ai_count}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4051d137",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5. TOKENIZATION\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
                "\n",
                "def tokenize_function(examples):\n",
                "    return tokenizer(\n",
                "        examples[\"text\"], \n",
                "        padding=\"max_length\", \n",
                "        truncation=True, \n",
                "        max_length=MAX_LEN\n",
                "    )\n",
                "\n",
                "print(\"Tokenizing dataset...\")\n",
                "tokenized_datasets = split_dataset.map(tokenize_function, batched=True)\n",
                "\n",
                "# Verify columns\n",
                "print(f\"Columns after tokenization: {tokenized_datasets['train'].column_names}\")\n",
                "\n",
                "# Set format for PyTorch - only include columns that exist\n",
                "required_cols = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
                "# token_type_ids may not exist for all models\n",
                "if \"token_type_ids\" in tokenized_datasets['train'].column_names:\n",
                "    required_cols.insert(1, \"token_type_ids\")\n",
                "\n",
                "tokenized_datasets.set_format(type=\"torch\", columns=required_cols)\n",
                "print(f\"Format set with columns: {required_cols}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ff1e1ce1",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 6. MODEL & TRAINER SETUP\n",
                "def compute_metrics(eval_pred):\n",
                "    logits, labels = eval_pred\n",
                "    predictions = np.argmax(logits, axis=1)\n",
                "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
                "    acc = accuracy_score(labels, predictions)\n",
                "    return {'accuracy': acc, 'f1': f1, 'precision': precision, 'recall': recall}\n",
                "\n",
                "model = AutoModelForSequenceClassification.from_pretrained(\n",
                "    MODEL_ID, \n",
                "    num_labels=2,\n",
                "    id2label={0: \"HUMAN\", 1: \"AI\"},\n",
                "    label2id={\"HUMAN\": 0, \"AI\": 1}\n",
                ")\n",
                "\n",
                "training_args = TrainingArguments(\n",
                "    output_dir=\"./checkpoints\",\n",
                "    learning_rate=LEARNING_RATE,\n",
                "    per_device_train_batch_size=BATCH_SIZE,\n",
                "    per_device_eval_batch_size=BATCH_SIZE,\n",
                "    num_train_epochs=EPOCHS,\n",
                "    weight_decay=0.01,\n",
                "    eval_strategy=\"epoch\",\n",
                "    save_strategy=\"epoch\",\n",
                "    load_best_model_at_end=True,\n",
                "    fp16=True, \n",
                "    report_to=\"none\"\n",
                ")\n",
                "\n",
                "trainer = Trainer(\n",
                "    model=model,\n",
                "    args=training_args,\n",
                "    train_dataset=tokenized_datasets[\"train\"],\n",
                "    eval_dataset=tokenized_datasets[\"test\"],\n",
                "    tokenizer=tokenizer,\n",
                "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
                "    compute_metrics=compute_metrics,\n",
                ")\n",
                "print(\"Trainer ready!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9584c421",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 7. TRAIN & SAVE\n",
                "print(\"Starting training...\")\n",
                "trainer.train()\n",
                "\n",
                "print(f\"\\nSaving model to: {OUTPUT_DIR}\")\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "trainer.save_model(OUTPUT_DIR)\n",
                "tokenizer.save_pretrained(OUTPUT_DIR)\n",
                "\n",
                "# Save metrics\n",
                "metrics = trainer.evaluate()\n",
                "print(f\"Final Metrics: {metrics}\")\n",
                "with open(os.path.join(OUTPUT_DIR, \"metrics.txt\"), \"w\") as f:\n",
                "    f.write(str(metrics))\n",
                "\n",
                "print(\"\\nâœ… Training Complete!\")\n",
                "print(f\"The model is saved at: {os.path.abspath(OUTPUT_DIR)}\")\n",
                "print(\"IMPORTANT: Download the folder from VS Code File Explorer to your local machine.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Post-Training Instructions\n",
                "Once you have downloaded the folder, place it in your local project at `backend/text_detector/model/`."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
