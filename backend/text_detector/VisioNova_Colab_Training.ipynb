{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07f30ba3",
   "metadata": {},
   "source": [
    "# üöÄ VisioNova: DeBERTa-v3 AI Text Detection Training\n",
    "\n",
    "This notebook trains a state-of-the-art `microsoft/deberta-v3-base` model to detect AI-generated text.\n",
    "All models and checkpoints are automatically saved to **Google Drive** for persistence.\n",
    "\n",
    "## ‚ö° Quick Start\n",
    "1. **Enable GPU**: Go to `Runtime > Change runtime type > T4 GPU` (or L4/A100 for faster training)\n",
    "2. **Mount Google Drive**: The notebook will prompt you to authorize access\n",
    "3. **Run all cells**: `Runtime > Run all`\n",
    "4. **Wait for training** (~2-3 hours on T4 GPU with full dataset)\n",
    "5. **Model is saved to**: `MyDrive/VisioNova_Models/`\n",
    "\n",
    "## üìÅ Google Drive Structure\n",
    "```\n",
    "MyDrive/\n",
    "‚îî‚îÄ‚îÄ VisioNova_Models/\n",
    "    ‚îî‚îÄ‚îÄ DeBERTa_v3_YYYYMMDD_HHMMSS/\n",
    "        ‚îú‚îÄ‚îÄ config.json\n",
    "        ‚îú‚îÄ‚îÄ model.safetensors\n",
    "        ‚îú‚îÄ‚îÄ tokenizer.json\n",
    "        ‚îú‚îÄ‚îÄ tokenizer_config.json\n",
    "        ‚îú‚îÄ‚îÄ special_tokens_map.json\n",
    "        ‚îú‚îÄ‚îÄ spm.model\n",
    "        ‚îú‚îÄ‚îÄ training_info.json\n",
    "        ‚îî‚îÄ‚îÄ checkpoints/\n",
    "            ‚îú‚îÄ‚îÄ checkpoint-epoch-1/\n",
    "            ‚îú‚îÄ‚îÄ checkpoint-epoch-2/\n",
    "            ‚îî‚îÄ‚îÄ checkpoint-epoch-3/\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbdfe66",
   "metadata": {},
   "source": [
    "## üì¶ 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4aa74dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ transformers\n",
      "‚úÖ datasets\n",
      "‚úÖ accelerate\n",
      "‚úÖ sklearn\n",
      "‚úÖ sentencepiece\n",
      "\n",
      "‚úÖ All dependencies installed!\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers datasets accelerate scikit-learn sentencepiece safetensors\n",
    "\n",
    "# Verify installations\n",
    "import importlib\n",
    "packages = ['transformers', 'datasets', 'accelerate', 'sklearn', 'sentencepiece']\n",
    "for pkg in packages:\n",
    "    try:\n",
    "        importlib.import_module(pkg)\n",
    "        print(f\"‚úÖ {pkg}\")\n",
    "    except ImportError:\n",
    "        print(f\"‚ùå {pkg} - Please reinstall\")\n",
    "\n",
    "print(\"\\n‚úÖ All dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b8be8a",
   "metadata": {},
   "source": [
    "## üîß 2. Mount Google Drive & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8adb49ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Using local storage (VS Code Colab extension mode)\n",
      "   Path: /content\n",
      "\n",
      "‚ö†Ô∏è  Note: Model will be saved locally.\n",
      "   You'll download it at the end of training.\n",
      "‚úÖ Storage configured!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# ==========================================\n",
    "# VS CODE COLAB EXTENSION - LOCAL STORAGE\n",
    "# ==========================================\n",
    "# Note: drive.mount is not supported in VS Code Colab extension\n",
    "# Model will be saved locally and can be downloaded at the end\n",
    "\n",
    "# Use local storage instead of Google Drive\n",
    "LOCAL_STORAGE = \"/content\"\n",
    "\n",
    "print(\"üìÅ Using local storage (VS Code Colab extension mode)\")\n",
    "print(f\"   Path: {LOCAL_STORAGE}\")\n",
    "print(\"\\n‚ö†Ô∏è  Note: Model will be saved locally.\")\n",
    "print(\"   You'll download it at the end of training.\")\n",
    "print(\"‚úÖ Storage configured!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3740da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ GPU Detected: Tesla T4\n",
      "   Memory: 15.8 GB\n",
      "   Recommended batch size: 8\n",
      "\n",
      "üìä Configuration:\n",
      "   Dataset: artem9k/ai-text-detection-pile\n",
      "   Model: microsoft/deberta-v3-base\n",
      "   Epochs: 3\n",
      "   Batch Size: 8\n",
      "   Learning Rate: 2e-05\n",
      "   Max Samples: All (Full Dataset)\n",
      "\n",
      "üíæ Save Locations (Local):\n",
      "   Model: /content/VisioNova_Models/DeBERTa_v3_20260130_084416\n",
      "   Checkpoints: /content/VisioNova_Models/DeBERTa_v3_20260130_084416/checkpoints\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# ==========================================\n",
    "# GPU CHECK - REQUIRED!\n",
    "# ==========================================\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\n",
    "        \"\\n\\nüõë GPU NOT DETECTED! üõë\\n\"\n",
    "        \"1. Go to: Runtime > Change runtime type\\n\"\n",
    "        \"2. Select 'T4 GPU' (or L4/A100) from Hardware Accelerator\\n\"\n",
    "        \"3. Click Save and re-run this cell\\n\"\n",
    "    )\n",
    "else:\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"‚úÖ GPU Detected: {gpu_name}\")\n",
    "    print(f\"   Memory: {gpu_memory:.1f} GB\")\n",
    "    \n",
    "    # Recommend batch size based on GPU memory\n",
    "    if gpu_memory >= 40:  # A100\n",
    "        recommended_batch = 32\n",
    "    elif gpu_memory >= 20:  # L4/A10\n",
    "        recommended_batch = 16\n",
    "    else:  # T4\n",
    "        recommended_batch = 8\n",
    "    print(f\"   Recommended batch size: {recommended_batch}\")\n",
    "\n",
    "# ==========================================\n",
    "# CONFIGURATION - MODIFY AS NEEDED\n",
    "# ==========================================\n",
    "\n",
    "# Dataset Options:\n",
    "# - \"artem9k/ai-text-detection-pile\" (1.5M samples, recommended for production)\n",
    "# - \"Hello-SimpleAI/HC3\" (Human ChatGPT Comparison, ~40k samples)\n",
    "# - \"aadityaubhat/GPT-wiki-intro\" (GPT vs Wikipedia, ~150k samples)\n",
    "DATASET_NAME = \"artem9k/ai-text-detection-pile\"\n",
    "\n",
    "# Model\n",
    "MODEL_ID = \"microsoft/deberta-v3-base\"\n",
    "\n",
    "# Training Hyperparameters\n",
    "EPOCHS = 3              # Number of training epochs (3-5 recommended)\n",
    "BATCH_SIZE = 8          # Reduce to 4 if you get OOM errors\n",
    "LEARNING_RATE = 2e-5    # Standard for fine-tuning\n",
    "MAX_LENGTH = 512        # Max token length (512 for DeBERTa)\n",
    "EVAL_SPLIT = 0.1        # 10% for validation\n",
    "WARMUP_RATIO = 0.1      # Warmup steps as ratio of total steps\n",
    "WEIGHT_DECAY = 0.01     # L2 regularization\n",
    "\n",
    "# Limit dataset size (set to None for full dataset)\n",
    "# Recommended: 50000 for quick test, 200000 for moderate, None for full\n",
    "MAX_SAMPLES = None  # e.g., 50000 for quick test, None for full\n",
    "\n",
    "# LOCAL paths (VS Code Colab extension doesn't support Google Drive)\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "MODEL_SAVE_DIR = f\"/content/VisioNova_Models/DeBERTa_v3_{timestamp}\"\n",
    "CHECKPOINT_DIR = f\"{MODEL_SAVE_DIR}/checkpoints\"\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"\\nüìä Configuration:\")\n",
    "print(f\"   Dataset: {DATASET_NAME}\")\n",
    "print(f\"   Model: {MODEL_ID}\")\n",
    "print(f\"   Epochs: {EPOCHS}\")\n",
    "print(f\"   Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"   Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"   Max Samples: {MAX_SAMPLES if MAX_SAMPLES else 'All (Full Dataset)'}\")\n",
    "print(f\"\\nüíæ Save Locations (Local):\")\n",
    "print(f\"   Model: {MODEL_SAVE_DIR}\")\n",
    "print(f\"   Checkpoints: {CHECKPOINT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7823099",
   "metadata": {},
   "source": [
    "## üìö 3. Load & Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "852a22a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading dataset: artem9k/ai-text-detection-pile...\n",
      "   This may take a few minutes for large datasets...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "849344e0f0bd44b6895472b81b89f49c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adbb3c27f16a4b13bdacfb68ae383d9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00007-bc5952582e004d(‚Ä¶):   0%|          | 0.00/758M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f4cab7e263f455a850fe4cdd6dd4bb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00001-of-00007-71c80017bc45f3(‚Ä¶):   0%|          | 0.00/318M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "910b6e7786834501abf89b040cde0f42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00002-of-00007-ee2d43f396e78f(‚Ä¶):   0%|          | 0.00/125M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7109e88869c6404da403bfd4421af589",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00003-of-00007-529931154b42b5(‚Ä¶):   0%|          | 0.00/137M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2b4b307f5c24e5691ddc476d98d8efb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00004-of-00007-b269dc49374a2c(‚Ä¶):   0%|          | 0.00/137M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "267d081018c24de882c76a21e1f87cf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00005-of-00007-3dce5e05ddbad7(‚Ä¶):   0%|          | 0.00/258M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f08cd5b990434b4a9aa832cd2709859d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00006-of-00007-3d8a471ba0cf1c(‚Ä¶):   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25e75d29d8f240b58439706d721b098d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1392522 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Raw dataset size: 1,392,522 samples\n",
      "   Columns: ['source', 'id', 'text']\n",
      "\n",
      "‚úÖ Dataset prepared!\n",
      "   Training samples: 1,253,269\n",
      "   Validation samples: 139,253\n",
      "\n",
      "üìù Sample data:\n",
      "   source: human\n",
      "   id: 284048\n",
      "   text: He's ultra big... and a Magnus...\n",
      "\n",
      "He looks very spot on to the cartoon look. He's big, bulky, and o...\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import json\n",
    "\n",
    "print(f\"üì• Loading dataset: {DATASET_NAME}...\")\n",
    "print(\"   This may take a few minutes for large datasets...\\n\")\n",
    "\n",
    "# Load dataset based on source\n",
    "if DATASET_NAME == \"custom\":\n",
    "    # Load custom dataset from Google Drive\n",
    "    print(f\"   Loading custom dataset from: {CUSTOM_DATASET_PATH}\")\n",
    "    if not os.path.exists(CUSTOM_DATASET_PATH):\n",
    "        raise FileNotFoundError(f\"Custom dataset not found: {CUSTOM_DATASET_PATH}\")\n",
    "    \n",
    "    extension = CUSTOM_DATASET_PATH.split('.')[-1]\n",
    "    if extension == 'json':\n",
    "        with open(CUSTOM_DATASET_PATH, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        train_data = Dataset.from_dict(data)\n",
    "    else:\n",
    "        dataset = load_dataset(extension, data_files={\"train\": CUSTOM_DATASET_PATH})\n",
    "        train_data = dataset['train']\n",
    "else:\n",
    "    # Load from Hugging Face Hub\n",
    "    dataset = load_dataset(DATASET_NAME)\n",
    "    \n",
    "    # Get the training split\n",
    "    if 'train' in dataset:\n",
    "        train_data = dataset['train']\n",
    "    else:\n",
    "        train_data = dataset[list(dataset.keys())[0]]\n",
    "\n",
    "print(f\"   Raw dataset size: {len(train_data):,} samples\")\n",
    "print(f\"   Columns: {train_data.column_names}\")\n",
    "\n",
    "# Limit samples if specified\n",
    "if MAX_SAMPLES and len(train_data) > MAX_SAMPLES:\n",
    "    print(f\"   Limiting to {MAX_SAMPLES:,} samples...\")\n",
    "    train_data = train_data.shuffle(seed=42).select(range(MAX_SAMPLES))\n",
    "\n",
    "# Split into train/validation\n",
    "split_dataset = train_data.train_test_split(test_size=EVAL_SPLIT, seed=42)\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset prepared!\")\n",
    "print(f\"   Training samples: {len(split_dataset['train']):,}\")\n",
    "print(f\"   Validation samples: {len(split_dataset['test']):,}\")\n",
    "\n",
    "# Show sample data\n",
    "print(f\"\\nüìù Sample data:\")\n",
    "sample = split_dataset['train'][0]\n",
    "for key, value in sample.items():\n",
    "    if isinstance(value, str) and len(value) > 100:\n",
    "        print(f\"   {key}: {value[:100]}...\")\n",
    "    else:\n",
    "        print(f\"   {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30ff26d",
   "metadata": {},
   "source": [
    "## üè∑Ô∏è 4. Process Labels\n",
    "\n",
    "We convert `source` to numeric `labels` (0=human, 1=ai).\n",
    "\n",
    "**IMPORTANT:** The `artem9k/ai-text-detection-pile` dataset has:\n",
    "\n",
    "- Column `text`: The text content  - Column `source`: Either `\"human\"` or `\"ai\"` (lowercase strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4ca42df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Dataset columns: ['source', 'id', 'text']\n",
      "üìù Unique source values found: ['ai', 'human']\n",
      "‚ÑπÔ∏è  Creating labels from 'source' column...\n",
      "   Mapping: 'human' -> 0, 'ai' -> 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42d72fab6cac4caca15e7401ce287d7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding labels:   0%|          | 0/1253269 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec3030b9c3d54aaa9ff8e88016473ae8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding labels:   0%|          | 0/139253 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Labels created successfully\n",
      "\n",
      "üîç Verifying labels...\n",
      "   Label type: int\n",
      "   Sample value: 0\n",
      "\n",
      "üìä Label Distribution:\n",
      "   Human (0): 925,299 (73.8%)\n",
      "   AI (1):    327,970 (26.2%)\n",
      "‚úÖ Labels verified: only 0 and 1 present\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# LABEL PROCESSING - CRITICAL SECTION\n",
    "# ==========================================\n",
    "# artem9k/ai-text-detection-pile dataset has:\n",
    "#   - \"text\" column: the actual text\n",
    "#   - \"source\" column: \"human\" or \"ai\" (string values)\n",
    "# We need integer labels for the Trainer\n",
    "\n",
    "cols = split_dataset['train'].column_names\n",
    "print(f\"üìã Dataset columns: {cols}\")\n",
    "\n",
    "# Show actual source values to verify\n",
    "if 'source' in cols:\n",
    "    sample_sources = list(set(split_dataset['train']['source'][:1000]))\n",
    "    print(f\"üìù Unique source values found: {sample_sources}\")\n",
    "\n",
    "# ==========================================\n",
    "# HANDLE DIFFERENT DATASET FORMATS\n",
    "# ==========================================\n",
    "\n",
    "if 'labels' in cols:\n",
    "    print(\"‚úÖ 'labels' column already exists\")\n",
    "    # Verify they are integers\n",
    "    sample_label = split_dataset['train']['labels'][0]\n",
    "    if isinstance(sample_label, str):\n",
    "        print(\"‚ö†Ô∏è  Converting string labels to integers...\")\n",
    "        def convert_labels(example):\n",
    "            label = str(example['labels']).lower().strip()\n",
    "            example['labels'] = 0 if label in ['human', '0'] else 1\n",
    "            return example\n",
    "        split_dataset = split_dataset.map(convert_labels, desc=\"Converting labels\")\n",
    "\n",
    "elif 'label' in cols:\n",
    "    print(\"‚úÖ Found 'label' column, renaming to 'labels'\")\n",
    "    split_dataset = split_dataset.rename_column('label', 'labels')\n",
    "    # Verify integer type\n",
    "    sample_label = split_dataset['train']['labels'][0]\n",
    "    if isinstance(sample_label, str):\n",
    "        print(\"‚ö†Ô∏è  Converting string labels to integers...\")\n",
    "        def convert_labels(example):\n",
    "            label = str(example['labels']).lower().strip()\n",
    "            example['labels'] = 0 if label in ['human', '0'] else 1\n",
    "            return example\n",
    "        split_dataset = split_dataset.map(convert_labels, desc=\"Converting labels\")\n",
    "\n",
    "elif 'source' in cols:\n",
    "    # MOST COMMON CASE for artem9k/ai-text-detection-pile\n",
    "    print(\"‚ÑπÔ∏è  Creating labels from 'source' column...\")\n",
    "    print(\"   Mapping: 'human' -> 0, 'ai' -> 1\")\n",
    "    \n",
    "    def add_labels_from_source(example):\n",
    "        \"\"\"\n",
    "        Convert source to numeric labels.\n",
    "        artem9k/ai-text-detection-pile has EXACTLY:\n",
    "          - \"human\" -> 0\n",
    "          - \"ai\" -> 1\n",
    "        \"\"\"\n",
    "        source = str(example.get('source', '')).lower().strip()\n",
    "        # EXACT MATCH - this dataset only has \"human\" or \"ai\"\n",
    "        example['labels'] = 0 if source == 'human' else 1\n",
    "        return example\n",
    "    \n",
    "    split_dataset = split_dataset.map(add_labels_from_source, desc=\"Adding labels\")\n",
    "    print(\"‚úÖ Labels created successfully\")\n",
    "\n",
    "elif 'generated' in cols:\n",
    "    print(\"‚úÖ Found 'generated' column, renaming to 'labels'\")\n",
    "    split_dataset = split_dataset.rename_column('generated', 'labels')\n",
    "\n",
    "else:\n",
    "    raise ValueError(\n",
    "        f\"‚ùå Cannot find label column!\\n\"\n",
    "        f\"   Available columns: {cols}\\n\"\n",
    "        f\"   Expected: 'labels', 'label', 'source', or 'generated'\"\n",
    "    )\n",
    "\n",
    "# ==========================================\n",
    "# VERIFY LABELS\n",
    "# ==========================================\n",
    "print(\"\\nüîç Verifying labels...\")\n",
    "\n",
    "# Check data type\n",
    "sample_label = split_dataset['train']['labels'][0]\n",
    "print(f\"   Label type: {type(sample_label).__name__}\")\n",
    "print(f\"   Sample value: {sample_label}\")\n",
    "\n",
    "# Count distribution\n",
    "train_labels = split_dataset['train']['labels']\n",
    "human_count = sum(1 for l in train_labels if l == 0)\n",
    "ai_count = sum(1 for l in train_labels if l == 1)\n",
    "total = human_count + ai_count\n",
    "\n",
    "print(f\"\\nüìä Label Distribution:\")\n",
    "print(f\"   Human (0): {human_count:,} ({human_count/total*100:.1f}%)\")\n",
    "print(f\"   AI (1):    {ai_count:,} ({ai_count/total*100:.1f}%)\")\n",
    "\n",
    "# Verify only 0 and 1 exist\n",
    "unique_labels = set(list(train_labels)[:10000])\n",
    "\n",
    "if unique_labels == {0, 1}:\n",
    "    print(\"‚úÖ Labels verified: only 0 and 1 present\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  WARNING: Unexpected label values: {unique_labels}\")\n",
    "\n",
    "# Class imbalance warning\n",
    "ratio = min(human_count, ai_count) / max(human_count, ai_count)\n",
    "if ratio < 0.3:\n",
    "    print(f\"\\n‚ö†Ô∏è  CLASS IMBALANCE (ratio: {ratio:.2f})\")\n",
    "    print(\"   Model should still train fine.\")\n",
    "    print(\"   This is expected for this dataset (~74% human, ~26% AI)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2201fb",
   "metadata": {},
   "source": [
    "## üî§ 5. Tokenization\n",
    "\n",
    "**Note:** We use `padding=False` here and let `DataCollatorWithPadding` handle dynamic padding at batch time. This is much faster than padding all sequences to max_length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "029a7d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî§ Loading tokenizer: microsoft/deberta-v3-base...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f3684aa58da46b88ecf9a70ccf2d87f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "064dd5e4d45744d4ac1cfda6242c15aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b78db550baa6436aa50d13f423454c0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/convert_slow_tokenizer.py:566: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Tokenizing dataset (this may take a while for large datasets)...\n",
      "   Removing columns: ['source', 'id', 'text']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bea36874ebb744f7b479566125ffa32b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/1253269 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed2e8fa974a14899888b6d62fc52d12b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/139253 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Columns after tokenization: ['labels', 'input_ids', 'token_type_ids', 'attention_mask']\n",
      "\n",
      "‚úÖ Tokenization complete!\n",
      "   Columns: ['labels', 'input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "print(f\"üî§ Loading tokenizer: {MODEL_ID}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize text - padding handled by DataCollator for efficiency.\"\"\"\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=False,  # DYNAMIC PADDING - DataCollator handles this\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH\n",
    "    )\n",
    "\n",
    "print(\"   Tokenizing dataset (this may take a while for large datasets)...\")\n",
    "\n",
    "# Get columns to remove (keep only 'labels')\n",
    "cols_to_remove = [col for col in split_dataset['train'].column_names if col != 'labels']\n",
    "print(f\"   Removing columns: {cols_to_remove}\")\n",
    "\n",
    "tokenized_datasets = split_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=cols_to_remove,\n",
    "    desc=\"Tokenizing\",\n",
    "    # NOTE: num_proc removed - can cause hangs on Colab with Drive\n",
    ")\n",
    "\n",
    "# Verify columns\n",
    "cols = tokenized_datasets[\"train\"].column_names\n",
    "print(f\"   Columns after tokenization: {cols}\")\n",
    "\n",
    "# Verify required columns exist\n",
    "required = ['input_ids', 'attention_mask', 'labels']\n",
    "for col in required:\n",
    "    if col not in cols:\n",
    "        raise ValueError(f\"Missing required column: {col}\")\n",
    "\n",
    "# NOTE: We don't call set_format() - DataCollatorWithPadding handles tensor conversion\n",
    "# This avoids potential issues with DatasetDict format persistence\n",
    "\n",
    "print(f\"\\n‚úÖ Tokenization complete!\")\n",
    "print(f\"   Columns: {cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c49402d",
   "metadata": {},
   "source": [
    "## üß† 6. Initialize Model & Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "666a2ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Loading model: microsoft/deberta-v3-base...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "341723c42f69429ba29f4dd00810f96b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/371M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b91d7e2f0f8f42ceba831231edd3cfa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/371M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Total parameters: 184,423,682\n",
      "   Trainable parameters: 184,423,682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-3827519430.py:77: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Model and Trainer initialized!\n",
      "   Checkpoints will be saved to: /content/VisioNova_Models/DeBERTa_v3_20260130_084416/checkpoints\n",
      "   Total training steps: ~469,974\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "import gc\n",
    "\n",
    "# Free up memory before loading model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average='binary', zero_division=0\n",
    "    )\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "print(f\"üß† Loading model: {MODEL_ID}...\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    num_labels=2,\n",
    "    id2label={0: \"HUMAN\", 1: \"AI\"},\n",
    "    label2id={\"HUMAN\": 0, \"AI\": 1}\n",
    ")\n",
    "\n",
    "# Print model size\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"   Total parameters: {total_params:,}\")\n",
    "print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Training arguments - CHECKPOINTS SAVED TO GOOGLE DRIVE\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=CHECKPOINT_DIR,  # Checkpoints saved to Drive!\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    \n",
    "    # Evaluation & Saving - All to Google Drive\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=3,  # Keep only last 3 checkpoints to save space\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    \n",
    "    # CRITICAL: Use bf16 instead of fp16 for DeBERTa-v3\n",
    "    # fp16 can cause gradient overflow/NaN losses with this model\n",
    "    fp16=False,\n",
    "    bf16=True,  # More stable for DeBERTa-v3 on T4/A100 GPUs\n",
    "    \n",
    "    dataloader_num_workers=2,\n",
    "    gradient_accumulation_steps=1,  # Increase to 2-4 if OOM with small batch size\n",
    "    \n",
    "    # Logging\n",
    "    logging_dir=f\"{MODEL_SAVE_DIR}/logs\",\n",
    "    logging_steps=100,\n",
    "    report_to=\"none\",\n",
    "    \n",
    "    # Disable hub push\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Model and Trainer initialized!\")\n",
    "\n",
    "total_steps = len(tokenized_datasets[\"train\"]) // BATCH_SIZE * EPOCHS\n",
    "print(f\"   Checkpoints will be saved to: {CHECKPOINT_DIR}\")\n",
    "print(f\"   Total training steps: ~{total_steps:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad107c34",
   "metadata": {},
   "source": [
    "## üß™ 7. Safety Check (Quick Validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e3cb145",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-3099541497.py:9: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  safety_trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Running safety check (10 samples, 2 steps)...\n",
      "   This verifies the pipeline works before full training.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:00, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.659100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.691700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Safety check PASSED!\n",
      "   ‚úì Model loads correctly\n",
      "   ‚úì Data pipeline works\n",
      "   ‚úì GPU computation functional\n",
      "   ‚úì Ready for full training!\n"
     ]
    }
   ],
   "source": [
    "print(\"üß™ Running safety check (10 samples, 2 steps)...\")\n",
    "print(\"   This verifies the pipeline works before full training.\\n\")\n",
    "\n",
    "try:\n",
    "    # Mini dataset for sanity check\n",
    "    mini_train = tokenized_datasets[\"train\"].select(range(min(10, len(tokenized_datasets[\"train\"]))))\n",
    "    mini_test = tokenized_datasets[\"test\"].select(range(min(10, len(tokenized_datasets[\"test\"]))))\n",
    "\n",
    "    safety_trainer = Trainer(\n",
    "        model=model,\n",
    "        args=TrainingArguments(\n",
    "            output_dir=\"./safety_check\",\n",
    "            max_steps=2,\n",
    "            per_device_train_batch_size=2,\n",
    "            logging_steps=1,\n",
    "            report_to=\"none\",\n",
    "            fp16=False,\n",
    "            bf16=True,  # Match main training - bf16 is more stable for DeBERTa\n",
    "            save_strategy=\"no\",  # Don't save during safety check\n",
    "        ),\n",
    "        train_dataset=mini_train,\n",
    "        eval_dataset=mini_test,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    safety_trainer.train()\n",
    "    \n",
    "    # Clean up safety check\n",
    "    import shutil\n",
    "    if os.path.exists(\"./safety_check\"):\n",
    "        shutil.rmtree(\"./safety_check\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Safety check PASSED!\")\n",
    "    print(\"   ‚úì Model loads correctly\")\n",
    "    print(\"   ‚úì Data pipeline works\")\n",
    "    print(\"   ‚úì GPU computation functional\")\n",
    "    print(\"   ‚úì Ready for full training!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Safety check FAILED: {e}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"   1. Check if GPU is enabled (Runtime > Change runtime type)\")\n",
    "    print(\"   2. Try reducing BATCH_SIZE to 4\")\n",
    "    print(\"   3. Check dataset format matches expected columns\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985d90c4",
   "metadata": {},
   "source": [
    "## üöÄ 8. Start Training\n",
    "\n",
    "‚ö†Ô∏è **Training Time Estimates:**\n",
    "- **T4 GPU**: ~2-4 hours (full dataset)\n",
    "- **L4 GPU**: ~1-2 hours (full dataset)  \n",
    "- **A100 GPU**: ~30-60 minutes (full dataset)\n",
    "\n",
    "Progress is shown below. **Checkpoints are automatically saved to Google Drive** after each epoch, so you won't lose progress if the session disconnects!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d90f9979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting training...\n",
      "   Epochs: 3\n",
      "   Training samples: 1,253,269\n",
      "   Batch size: 8\n",
      "   Checkpoints saving to: /content/VisioNova_Models/DeBERTa_v3_20260130_084416/checkpoints\n",
      "\n",
      "‚è±Ô∏è  Estimated time: 2-4 hours on T4 GPU\n",
      "\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='469977' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    50/469977 01:02 < 170:25:17, 0.77 it/s, Epoch 0.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "‚ö†Ô∏è Training interrupted!\n",
      "   Checkpoints saved to: /content/VisioNova_Models/DeBERTa_v3_20260130_084416/checkpoints\n",
      "   You can resume training from the latest checkpoint.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-406542507.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Train!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mtrain_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\\n‚ö†Ô∏è Training interrupted!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2324\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2325\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2326\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2327\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2672\u001b[0m                     )\n\u001b[1;32m   2673\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2674\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2676\u001b[0m                     if (\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   4069\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"scale_wrt_gas\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4071\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4072\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4073\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2850\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlomo_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2851\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2852\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2854\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_trigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             )\n\u001b[0;32m--> 625\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "print(\"üöÄ Starting training...\")\n",
    "print(f\"   Epochs: {EPOCHS}\")\n",
    "print(f\"   Training samples: {len(tokenized_datasets['train']):,}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   Checkpoints saving to: {CHECKPOINT_DIR}\")\n",
    "print(f\"\\n‚è±Ô∏è  Estimated time: 2-4 hours on T4 GPU\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Train!\n",
    "try:\n",
    "    train_result = trainer.train()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n\\n‚ö†Ô∏è Training interrupted!\")\n",
    "    print(f\"   Checkpoints saved to: {CHECKPOINT_DIR}\")\n",
    "    print(\"   You can resume training from the latest checkpoint.\")\n",
    "    raise\n",
    "\n",
    "# Calculate training time\n",
    "training_time = time.time() - start_time\n",
    "hours, remainder = divmod(training_time, 3600)\n",
    "minutes, seconds = divmod(remainder, 60)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n‚úÖ Training complete!\")\n",
    "print(f\"   Total time: {int(hours)}h {int(minutes)}m {int(seconds)}s\")\n",
    "print(f\"   Final training loss: {train_result.training_loss:.4f}\")\n",
    "print(f\"   Checkpoints saved to: {CHECKPOINT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9120ff21",
   "metadata": {},
   "source": [
    "## üìä 9. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf49100",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"üìä Evaluating model on validation set...\")\n",
    "\n",
    "metrics = trainer.evaluate()\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"üìà FINAL METRICS\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"   Accuracy:  {metrics['eval_accuracy']:.4f} ({metrics['eval_accuracy']*100:.2f}%)\")\n",
    "print(f\"   F1 Score:  {metrics['eval_f1']:.4f}\")\n",
    "print(f\"   Precision: {metrics['eval_precision']:.4f}\")\n",
    "print(f\"   Recall:    {metrics['eval_recall']:.4f}\")\n",
    "print(f\"   Loss:      {metrics['eval_loss']:.4f}\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "# Get predictions for confusion matrix\n",
    "print(\"\\nüìâ Generating confusion matrix...\")\n",
    "predictions = trainer.predict(tokenized_datasets[\"test\"])\n",
    "preds = np.argmax(predictions.predictions, axis=1)\n",
    "labels = predictions.label_ids\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(labels, preds)\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"                 Predicted\")\n",
    "print(f\"              HUMAN    AI\")\n",
    "print(f\"Actual HUMAN   {cm[0][0]:5d}  {cm[0][1]:5d}\")\n",
    "print(f\"       AI      {cm[1][0]:5d}  {cm[1][1]:5d}\")\n",
    "\n",
    "# Classification report\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(labels, preds, target_names=['HUMAN', 'AI']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f872b180",
   "metadata": {},
   "source": [
    "## üíæ 10. Save Final Model to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8566a634",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "print(f\"üíæ Saving final model...\")\n",
    "print(f\"   Path: {MODEL_SAVE_DIR}\\n\")\n",
    "\n",
    "# Save model and tokenizer\n",
    "trainer.save_model(MODEL_SAVE_DIR)\n",
    "tokenizer.save_pretrained(MODEL_SAVE_DIR)\n",
    "\n",
    "# Save comprehensive training info\n",
    "training_info = {\n",
    "    \"model_name\": \"VisioNova AI Text Detector\",\n",
    "    \"base_model\": MODEL_ID,\n",
    "    \"dataset\": DATASET_NAME,\n",
    "    \"training_config\": {\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"max_length\": MAX_LENGTH,\n",
    "        \"warmup_ratio\": WARMUP_RATIO,\n",
    "        \"weight_decay\": WEIGHT_DECAY,\n",
    "    },\n",
    "    \"dataset_info\": {\n",
    "        \"training_samples\": len(tokenized_datasets['train']),\n",
    "        \"validation_samples\": len(tokenized_datasets['test']),\n",
    "        \"max_samples_limit\": MAX_SAMPLES,\n",
    "    },\n",
    "    \"final_metrics\": {\n",
    "        \"accuracy\": float(metrics['eval_accuracy']),\n",
    "        \"f1\": float(metrics['eval_f1']),\n",
    "        \"precision\": float(metrics['eval_precision']),\n",
    "        \"recall\": float(metrics['eval_recall']),\n",
    "        \"loss\": float(metrics['eval_loss'])\n",
    "    },\n",
    "    \"training_info\": {\n",
    "        \"training_time_seconds\": training_time,\n",
    "        \"training_time_formatted\": f\"{int(hours)}h {int(minutes)}m {int(seconds)}s\",\n",
    "        \"final_training_loss\": float(train_result.training_loss),\n",
    "    },\n",
    "    \"environment\": {\n",
    "        \"gpu\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\",\n",
    "        \"gpu_memory_gb\": torch.cuda.get_device_properties(0).total_memory / 1e9 if torch.cuda.is_available() else 0,\n",
    "        \"pytorch_version\": torch.__version__,\n",
    "    },\n",
    "    \"timestamp\": timestamp,\n",
    "    \"labels\": {\n",
    "        \"0\": \"HUMAN\",\n",
    "        \"1\": \"AI\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save training info JSON\n",
    "info_path = os.path.join(MODEL_SAVE_DIR, \"training_info.json\")\n",
    "with open(info_path, \"w\") as f:\n",
    "    json.dump(training_info, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Model saved!\")\n",
    "print(f\"\\nüìÅ Saved files:\")\n",
    "total_size = 0\n",
    "for f in sorted(os.listdir(MODEL_SAVE_DIR)):\n",
    "    filepath = os.path.join(MODEL_SAVE_DIR, f)\n",
    "    if os.path.isfile(filepath):\n",
    "        size = os.path.getsize(filepath) / 1e6\n",
    "        total_size += size\n",
    "        print(f\"   üìÑ {f} ({size:.1f} MB)\")\n",
    "    else:\n",
    "        print(f\"   üìÅ {f}/ (directory)\")\n",
    "\n",
    "print(f\"\\n   Total size: {total_size:.1f} MB\")\n",
    "print(f\"\\n‚ö†Ô∏è  IMPORTANT: Run the next cell to download the model!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0460bd3a",
   "metadata": {},
   "source": [
    "## üß™ 11. Test the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d59d3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "print(\"üß™ Testing the trained model...\\n\")\n",
    "\n",
    "# Create inference pipeline from saved model\n",
    "classifier = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=MODEL_SAVE_DIR,\n",
    "    tokenizer=MODEL_SAVE_DIR,\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "# Test samples - mix of human and AI-generated text\n",
    "test_texts = [\n",
    "    # Human-like texts\n",
    "    \"The quick brown fox jumps over the lazy dog. This is a simple sentence written by a human.\",\n",
    "    \"Yesterday I went to the store and bought some groceries. The weather was nice so I walked.\",\n",
    "    \"Climate change is caused by the increase in greenhouse gases in the atmosphere, primarily from burning fossil fuels.\",\n",
    "    \"I can't believe how much prices have gone up lately. Everything is so expensive now!\",\n",
    "    \n",
    "    # AI-like texts\n",
    "    \"As an AI language model, I cannot provide assistance with that request. However, I can help you understand the concept better.\",\n",
    "    \"I apologize, but I'm unable to fulfill this request as it goes against my ethical guidelines.\",\n",
    "    \"In conclusion, it is important to note that there are several factors to consider when examining this complex issue.\",\n",
    "    \"The implications of this development are multifaceted and warrant careful consideration from multiple perspectives.\",\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'TEXT':<50} {'PREDICTION':<10} {'CONFIDENCE':<10}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for text in test_texts:\n",
    "    result = classifier(text[:512])[0]  # Truncate to max length\n",
    "    label = result['label']\n",
    "    confidence = result['score'] * 100\n",
    "\n",
    "    # Emoji based on prediction\n",
    "    emoji = \"ü§ñ\" if label == \"AI\" else \"üë§\"\n",
    "    \n",
    "    # Truncate text for display\n",
    "    display_text = text[:47] + \"...\" if len(text) > 50 else text\n",
    "    \n",
    "    print(f\"{display_text:<50} {emoji} {label:<8} {confidence:.1f}%\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"\\n‚úÖ Model testing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a623faa0",
   "metadata": {},
   "source": [
    "## üì• 12. Download Model\n",
    "\n",
    "Your model is already saved to Google Drive and will persist even after the Colab session ends!\n",
    "\n",
    "### Option A: Access via Google Drive (Recommended)\n",
    "1. Open [Google Drive](https://drive.google.com)\n",
    "2. Navigate to `My Drive/VisioNova_Models/`\n",
    "3. Find the folder with your training timestamp\n",
    "4. Right-click and select **Download** to get a zip file\n",
    "\n",
    "### Option B: Download directly from Colab\n",
    "Run the cell below to create a zip file and download it directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec05eb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Create zip file for download\n",
    "zip_name = f\"visionova_model_{timestamp}\"\n",
    "zip_path = f\"/content/{zip_name}\"\n",
    "\n",
    "print(f\"üì¶ Creating zip file for download...\")\n",
    "print(f\"   Source: {MODEL_SAVE_DIR}\")\n",
    "\n",
    "# Create zip (excluding checkpoints to reduce size)\n",
    "shutil.make_archive(zip_path, 'zip', MODEL_SAVE_DIR)\n",
    "\n",
    "zip_file = f\"{zip_path}.zip\"\n",
    "zip_size = os.path.getsize(zip_file) / 1e6\n",
    "print(f\"\\n‚úÖ Zip created: {zip_name}.zip\")\n",
    "print(f\"   Size: {zip_size:.1f} MB\")\n",
    "\n",
    "# Try to download (may not work in VS Code extension)\n",
    "try:\n",
    "    from google.colab import files\n",
    "    print(\"\\nüì• Starting download...\")\n",
    "    files.download(zip_file)\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è  Auto-download not available in VS Code extension\")\n",
    "    print(f\"\\nüì• To download your model:\")\n",
    "    print(f\"   1. Open the Colab file browser (folder icon on left)\")\n",
    "    print(f\"   2. Navigate to: /content/\")\n",
    "    print(f\"   3. Right-click on '{zip_name}.zip'\")\n",
    "    print(f\"   4. Select 'Download'\")\n",
    "    print(f\"\\n   Or run this in a new cell:\")\n",
    "    print(f\"   !cp {zip_file} /content/drive/MyDrive/  # If you mount Drive in browser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bc6408",
   "metadata": {},
   "source": [
    "## üîÑ 13. Resume Training from Checkpoint (Optional)\n",
    "\n",
    "If your Colab session disconnected during training, you can resume from the last checkpoint saved to Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca15fc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# RESUME TRAINING FROM CHECKPOINT\n",
    "# ==========================================\n",
    "# Uncomment and run this cell ONLY if you need to resume training\n",
    "# after a session disconnect.\n",
    "\n",
    "\"\"\"\n",
    "# Find the latest checkpoint\n",
    "import glob\n",
    "\n",
    "checkpoint_pattern = f\"{CHECKPOINT_DIR}/checkpoint-*\"\n",
    "checkpoints = glob.glob(checkpoint_pattern)\n",
    "\n",
    "if checkpoints:\n",
    "    # Sort by checkpoint number and get the latest\n",
    "    latest_checkpoint = max(checkpoints, key=lambda x: int(x.split('-')[-1]))\n",
    "    print(f\"üìç Found checkpoint: {latest_checkpoint}\")\n",
    "    \n",
    "    # Resume training\n",
    "    print(\"üîÑ Resuming training from checkpoint...\")\n",
    "    trainer.train(resume_from_checkpoint=latest_checkpoint)\n",
    "    \n",
    "    print(\"‚úÖ Training resumed and completed!\")\n",
    "else:\n",
    "    print(\"‚ùå No checkpoints found. Start fresh training instead.\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"‚ÑπÔ∏è  This cell is for resuming interrupted training.\")\n",
    "print(\"   Uncomment the code above if you need to resume from a checkpoint.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee00154",
   "metadata": {},
   "source": [
    "## ‚úÖ Training Complete!\n",
    "\n",
    "### üéâ Your model has been saved to Google Drive!\n",
    "\n",
    "**Location:** `My Drive/VisioNova_Models/DeBERTa_v3_YYYYMMDD_HHMMSS/`\n",
    "\n",
    "### üìÅ Model Files Structure:\n",
    "```\n",
    "VisioNova_Models/\n",
    "‚îî‚îÄ‚îÄ DeBERTa_v3_YYYYMMDD_HHMMSS/\n",
    "    ‚îú‚îÄ‚îÄ config.json              # Model configuration\n",
    "    ‚îú‚îÄ‚îÄ model.safetensors        # Model weights (~700MB)\n",
    "    ‚îú‚îÄ‚îÄ tokenizer.json           # Tokenizer data\n",
    "    ‚îú‚îÄ‚îÄ tokenizer_config.json    # Tokenizer configuration\n",
    "    ‚îú‚îÄ‚îÄ special_tokens_map.json  # Special tokens\n",
    "    ‚îú‚îÄ‚îÄ spm.model                # SentencePiece model\n",
    "    ‚îú‚îÄ‚îÄ training_info.json       # Training metadata & metrics\n",
    "    ‚îî‚îÄ‚îÄ checkpoints/             # Epoch checkpoints\n",
    "        ‚îú‚îÄ‚îÄ checkpoint-XXX/\n",
    "        ‚îî‚îÄ‚îÄ ...\n",
    "```\n",
    "\n",
    "### üöÄ Next Steps:\n",
    "\n",
    "1. **Download the model** from Google Drive to your local machine\n",
    "\n",
    "2. **Copy files to your VisioNova project:**\n",
    "   ```\n",
    "   backend/text_detector/model/\n",
    "   ‚îú‚îÄ‚îÄ config.json\n",
    "   ‚îú‚îÄ‚îÄ model.safetensors\n",
    "   ‚îú‚îÄ‚îÄ tokenizer.json\n",
    "   ‚îú‚îÄ‚îÄ tokenizer_config.json\n",
    "   ‚îú‚îÄ‚îÄ special_tokens_map.json\n",
    "   ‚îî‚îÄ‚îÄ spm.model\n",
    "   ```\n",
    "\n",
    "3. **Restart your VisioNova backend** to use the new model\n",
    "\n",
    "### üìä Training Summary:\n",
    "- **Dataset:** Check `training_info.json` for details\n",
    "- **Metrics:** F1, Accuracy, Precision, Recall saved in `training_info.json`\n",
    "- **Checkpoints:** Available in `checkpoints/` folder for resuming or rollback\n",
    "\n",
    "---\n",
    "*VisioNova AI Text Detection - Trained with DeBERTa-v3*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
